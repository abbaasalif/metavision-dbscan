{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "torch.backends.cudnn.benchmark=True\n",
    "from catalyst import dl\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from catalyst.contrib.nn.criterion.iou import IoULoss\n",
    "from catalyst.contrib.nn.criterion.dice import DiceLoss\n",
    "from catalyst.contrib.nn.criterion.trevsky import TrevskyLoss\n",
    "import torchvision.transforms.functional as TF\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Optional\n",
    "from PIL import Image\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform(object):\n",
    "  \"\"\" Rescale the image in a sample to a given size.\n",
    "\n",
    "  Args:\n",
    "    output_size (tuple or int): Desired output size. If tuple, output is\n",
    "    matched to output_size. If int, smaller of image edges is matched.\n",
    "\n",
    "    crop_size (tuple or int): Desired size for random cropping the image.\n",
    "  \"\"\"\n",
    "  def __init__(self, output_size, crop_size):\n",
    "    assert isinstance(output_size, (int, tuple))\n",
    "    assert isinstance(crop_size, (int, tuple))\n",
    "    self.output_size = output_size\n",
    "    self.crop_size = crop_size\n",
    "\n",
    "  def __call__(self, sample):\n",
    "    image, mask = sample['image'], sample['mask']\n",
    "    #Resize\n",
    "    resize = transforms.Resize(self.output_size)\n",
    "    image = resize(image)\n",
    "    mask = resize(mask)\n",
    "\n",
    "    #Random crop\n",
    "    i, j, h, w = transforms.RandomCrop.get_params(image, output_size=self.crop_size)\n",
    "    image = TF.crop(image, i, j, h, w)\n",
    "    mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "    #Random horizontal flipping\n",
    "    if random.random() > 0.5:\n",
    "      image = TF.hflip(image)\n",
    "      mask = TF.hflip(mask)\n",
    "\n",
    "    #Random vertical flipping\n",
    "    if random.random() > 0.5:\n",
    "      image = TF.vflip(image)\n",
    "      mask = TF.vflip(mask)\n",
    "    image = TF.to_tensor(image)\n",
    "    mask = TF.to_tensor(mask)\n",
    "\n",
    "    return {'image': image, 'mask': mask}\n",
    "\n",
    "class SegmentationDataset(VisionDataset):\n",
    "    \"\"\"A PyTorch dataset for image segmentation task.\n",
    "    The dataset is compatible with torchvision transforms.\n",
    "    The transforms passed would be applied to both the Images and Masks.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 image_folder: str,\n",
    "                 mask_folder: str,\n",
    "                 transforms: Optional[Callable] = None,\n",
    "                 seed: int = None,\n",
    "                 fraction: float = None,\n",
    "                 subset: str = None,\n",
    "                 train_set: bool = False,\n",
    "                 image_color_mode: str = \"rgb\",\n",
    "                 mask_color_mode: str = \"grayscale\") -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory path.\n",
    "            image_folder (str): Name of the folder that contains the images in the root directory.\n",
    "            mask_folder (str): Name of the folder that contains the masks in the root directory.\n",
    "            transforms (Optional[Callable], optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.ToTensor`` for images. Defaults to None.\n",
    "            seed (int, optional): Specify a seed for the train and test split for reproducible results. Defaults to None.\n",
    "            fraction (float, optional): A float value from 0 to 1 which specifies the validation split fraction. Defaults to None.\n",
    "            subset (str, optional): 'Train' or 'Test' to select the appropriate set. Defaults to None.\n",
    "            image_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'rgb'.\n",
    "            mask_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'grayscale'.\n",
    "\n",
    "        Raises:\n",
    "            OSError: If image folder doesn't exist in root.\n",
    "            OSError: If mask folder doesn't exist in root.\n",
    "            ValueError: If subset is not either 'Train' or 'Test'\n",
    "            ValueError: If image_color_mode and mask_color_mode are either 'rgb' or 'grayscale'\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms)\n",
    "        image_folder_path = Path(self.root) / image_folder\n",
    "        mask_folder_path = Path(self.root) / mask_folder\n",
    "        if not image_folder_path.exists():\n",
    "            raise OSError(f\"{image_folder_path} does not exist.\")\n",
    "        if not mask_folder_path.exists():\n",
    "            raise OSError(f\"{mask_folder_path} does not exist.\")\n",
    "\n",
    "        if image_color_mode not in [\"rgb\", \"grayscale\"]:\n",
    "            raise ValueError(\n",
    "                f\"{image_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
    "            )\n",
    "        if mask_color_mode not in [\"rgb\", \"grayscale\"]:\n",
    "            raise ValueError(\n",
    "                f\"{mask_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
    "            )\n",
    "\n",
    "        self.image_color_mode = image_color_mode\n",
    "        self.mask_color_mode = mask_color_mode\n",
    "\n",
    "        if not fraction:\n",
    "            self.image_names = sorted(image_folder_path.glob(\"*\"))\n",
    "            self.mask_names = sorted(mask_folder_path.glob(\"*\"))\n",
    "        else:\n",
    "            if subset not in [\"Train\", \"Test\"]:\n",
    "                raise (ValueError(\n",
    "                    f\"{subset} is not a valid input. Acceptable values are Train and Test.\"\n",
    "                ))\n",
    "            self.fraction = fraction\n",
    "            self.image_list = np.array(sorted(image_folder_path.glob(\"*\")))\n",
    "            self.mask_list = np.array(sorted(mask_folder_path.glob(\"*\")))\n",
    "            if seed:\n",
    "                np.random.seed(seed)\n",
    "                indices = np.arange(len(self.image_list))\n",
    "                np.random.shuffle(indices)\n",
    "                self.image_list = self.image_list[indices]\n",
    "                self.mask_list = self.mask_list[indices]\n",
    "            if subset == \"Train\":\n",
    "                self.image_names = self.image_list[:int(\n",
    "                    np.ceil(len(self.image_list) * (1 - self.fraction)))]\n",
    "                self.mask_names = self.mask_list[:int(\n",
    "                    np.ceil(len(self.mask_list) * (1 - self.fraction)))]\n",
    "            else:\n",
    "                self.image_names = self.image_list[\n",
    "                    int(np.ceil(len(self.image_list) * (1 - self.fraction))):]\n",
    "                self.mask_names = self.mask_list[\n",
    "                    int(np.ceil(len(self.mask_list) * (1 - self.fraction))):]\n",
    "        self.train_set = train_set\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        image_path = self.image_names[index]\n",
    "        mask_path = self.mask_names[index]\n",
    "        with open(image_path, \"rb\") as image_file, open(mask_path,\n",
    "                                                        \"rb\") as mask_file:\n",
    "            image = Image.open(image_file)\n",
    "            if self.image_color_mode == \"rgb\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            elif self.image_color_mode == \"grayscale\":\n",
    "                image = image.convert(\"L\")\n",
    "            mask = Image.open(mask_file)\n",
    "            if self.mask_color_mode == \"rgb\":\n",
    "                mask = mask.convert(\"RGB\")\n",
    "            elif self.mask_color_mode == \"grayscale\":\n",
    "                mask = mask.convert(\"L\")\n",
    "            sample = {\"image\": image, \"mask\": mask}\n",
    "            if self.transforms and self.train_set:\n",
    "              sample = self.transforms(sample)\n",
    "            elif self.transforms:\n",
    "                sample[\"image\"] = self.transforms(sample[\"image\"])\n",
    "                sample[\"mask\"] = self.transforms(sample[\"mask\"])\n",
    "        return sample\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "       CustomTransform(output_size=(520, 520), crop_size=(512, 512))\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((512, 512)), transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = SegmentationDataset('./segmentation_new', 'images','masks', transforms=train_transforms, image_color_mode='rgb', mask_color_mode='rgb', train_set=True)\n",
    "val_dataset = SegmentationDataset('./segmentation_new', 'images', 'masks', transforms=val_transforms, image_color_mode='rgb', mask_color_mode='rgb')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=1, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 9000) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m runner \u001b[38;5;241m=\u001b[39m CustomRunner(input_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, output_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m, target_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#model training\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIOUCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiceCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrevskyCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscores\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStoppingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./segementation_deeplab_resnet50_iou\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminimize_valid_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_on_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     77\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\runners\\runner.py:514\u001b[0m, in \u001b[0;36mRunner.train\u001b[1;34m(self, loaders, model, engine, trial, criterion, optimizer, scheduler, callbacks, loggers, seed, hparams, num_epochs, logdir, valid_loader, valid_metric, minimize_valid_metric, verbose, timeit, check, overfit, load_best_on_end, fp16, amp, apex, ddp)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_best_on_end \u001b[38;5;241m=\u001b[39m load_best_on_end\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m--> 514\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:828\u001b[0m, in \u001b[0;36mIRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m ex\n\u001b[1;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_exception\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:751\u001b[0m, in \u001b[0;36mIRunner._run_event\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(callback, event)(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_str_intersections(event, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_end\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_exception\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 751\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:743\u001b[0m, in \u001b[0;36mIRunner.on_exception\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_exception\u001b[39m(\u001b[38;5;28mself\u001b[39m, runner: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRunner\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    742\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Event handler.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 743\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:825\u001b[0m, in \u001b[0;36mIRunner.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the experiment.\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    self, `IRunner` instance after the experiment\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:815\u001b[0m, in \u001b[0;36mIRunner._run_experiment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    810\u001b[0m         torch\u001b[38;5;241m.\u001b[39mmultiprocessing\u001b[38;5;241m.\u001b[39mspawn(\n\u001b[0;32m    811\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage, args\u001b[38;5;241m=\u001b[39m(world_size,), nprocs\u001b[38;5;241m=\u001b[39mworld_size, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    812\u001b[0m         )\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# single-device branch (cpu, gpu, dp)\u001b[39;00m\n\u001b[1;32m--> 815\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_experiment_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:798\u001b[0m, in \u001b[0;36mIRunner._run_stage\u001b[1;34m(self, rank, world_size)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_stage_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_epoch_step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_epoch_len:\n\u001b[1;32m--> 798\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_early_stop:\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneed_early_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:787\u001b[0m, in \u001b[0;36mIRunner._run_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaders\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\catalyst\\core\\runner.py:776\u001b[0m, in \u001b[0;36mIRunner._run_loader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_loader_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train_loader):\n\u001b[1;32m--> 776\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader):\n\u001b[0;32m    777\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m    778\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_batch()\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\alifb\\anaconda3\\envs\\seg_model\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 9000) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "class SegModel(torch.nn.Module):\n",
    "  \"\"\"\n",
    "    Adapted from\n",
    "    https://github.com/msminhas93/DeepLabv3FineTuning/blob/64541451e85d61dea66/model.py\n",
    "\n",
    "    And from looking at the source code for the deeplabv3_resnet50 module:\n",
    "\n",
    "    import inspect\n",
    "    from torchvision. import models\n",
    "\n",
    "    model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "    print(inspect.getsource(model.backbone.forward))\n",
    "    print(inspect.getsource(model.forward))\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, backbone='resnet'):\n",
    "    super().__init__()\n",
    "    self.backbone = backbone\n",
    "    if backbone == 'resnet':\n",
    "      deeplabv3_model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "      self.head = DeepLabHead(2048, 1)\n",
    "    else:\n",
    "      deeplabv3_model = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
    "      self.head = DeepLabHead(960, 1)\n",
    "\n",
    "    self.backbone = deeplabv3_model.backbone\n",
    "    self.backbone.trainable = False\n",
    "    self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # see 'inspect.getsource(deeplabv3_model.forward)'\n",
    "    input_shape = x.shape[-2:]\n",
    "    x = self.backbone(x)['out']\n",
    "    x = self.head(x)\n",
    "    x = torch.nn.functional.interpolate(\n",
    "        x, size=input_shape, mode='bilinear', align_corners=False\n",
    "    )\n",
    "    return self.sigmoid(x)\n",
    "\n",
    "model = SegModel(backbone='resnet')\n",
    "\n",
    "criterion = IoULoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "loaders = {'train': train_dataloader, 'valid':val_dataloader}\n",
    "\n",
    "class CustomRunner(dl.SupervisedRunner):\n",
    "  def handle_batch(self, batch):\n",
    "    x = batch['image']\n",
    "    y_pred = self.model(x)\n",
    "    target = batch['mask']\n",
    "    self.batch = {self._input_key:x, self._output_key: y_pred, self._target_key: target}\n",
    "\n",
    "runner = CustomRunner(input_key='features', output_key='scores', target_key='target', loss_key='loss')\n",
    "#model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion = criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    num_epochs=100,\n",
    "    callbacks=[\n",
    "        dl.IOUCallback(input_key='scores', target_key='target'),\n",
    "        dl.DiceCallback(input_key='scores', target_key='target'),\n",
    "        dl.TrevskyCallback(input_key='scores', target_key='target', alpha=0.2),\n",
    "        dl.EarlyStoppingCallback(patience=3, loader_key='valid', metric_key='loss', minimize=True),\n",
    "    ],\n",
    "    logdir='./segementation_deeplab_resnet50_iou',\n",
    "    valid_loader='valid',\n",
    "    valid_metric='loss',\n",
    "    minimize_valid_metric=True,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
